{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pickle\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(\"dataset_train_pp.csv\")\n",
    "test_set = pd.read_csv(\"dataset_test_pp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating input and label\n",
    "train_x=train_set[\"Description\"]\n",
    "test_x=test_set[\"Description\"]\n",
    "\n",
    "train_y=train_set[\"Class Index\"]\n",
    "test_y=test_set[\"Class Index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.6 s, sys: 1.81 s, total: 30.4 s\n",
      "Wall time: 34.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    \"GoogleNews-vectors-negative300.bin.gz\", binary=True, limit=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute L2-normalized vectors. You cannot continue training after doing a replace. \n",
    "# The model becomes effectively read-only: you can call most_similar(), similarity(), etc., but not train.\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['recipes',\n",
       " 'Gordon_Engelhardt',\n",
       " 'Alan_Sayre',\n",
       " 'newly_christened',\n",
       " 'dunking_sensation',\n",
       " 'whimsicality',\n",
       " 'Kortrijk',\n",
       " 'Environment_Rating_CBBER',\n",
       " 'Koukalova',\n",
       " 'Woodburn_Ind.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view some vocabularies\n",
    "list(islice(wv.vocab, 10, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    tokens_list = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            tokens_list.append(word)\n",
    "    return tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.7 s, sys: 190 ms, total: 43.9 s\n",
      "Wall time: 44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_x_t = test_x.apply(tokenizer).tolist()\n",
    "train_x_t = train_x.apply(tokenizer).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wv.vectors_norm or wv.syn0norm: unit-normaliyed vector of a vocabulary word, returns a 300 dim vector\n",
    "# gensim.matutils.unitvec : Scale a vector to unit length.\n",
    "def word_averaging(wv, words_list):\n",
    "    avg_all = []\n",
    "    unk = 0\n",
    "    for words in words_list:\n",
    "        norm_words = []\n",
    "        for token in words:\n",
    "            if token in wv.vocab:\n",
    "                norm_tokens = wv.vectors_norm[wv.vocab[token].index]\n",
    "                norm_words.append(norm_tokens)\n",
    "            else:\n",
    "                norm_words.append(np.zeros(300))\n",
    "                unk += 1\n",
    "                \n",
    "        avg_words = gensim.matutils.unitvec(abs(np.array(norm_words).mean(axis=0))).astype(np.float32)   \n",
    "        avg_all.append(avg_words)\n",
    "    print(\"No. of unknown words:\", unk)\n",
    "    return np.array(avg_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of unknown words: 26086\n",
      "CPU times: user 1.72 s, sys: 7.89 ms, total: 1.73 s\n",
      "Wall time: 1.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w2v_test_x = word_averaging(wv, test_x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of unknown words: 261172\n",
      "CPU times: user 15.9 s, sys: 98.9 ms, total: 16 s\n",
      "Wall time: 16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w2v_train_x = word_averaging(wv, train_x_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save w2v matrix to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"w2v_train_x.npy\", w2v_train_x)\n",
    "np.save(\"w2v_test_x.npy\", w2v_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test_x = np.load(\"w2v_test_x.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train_x = np.load(\"w2v_train_x.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03507248, 0.02853259, 0.02540481, ..., 0.06819418, 0.00254362,\n",
       "        0.01496225],\n",
       "       [0.02973051, 0.04870571, 0.01827599, ..., 0.02830017, 0.00388521,\n",
       "        0.01688914],\n",
       "       [0.02775904, 0.01455307, 0.00308727, ..., 0.05806882, 0.05713799,\n",
       "        0.08039328],\n",
       "       ...,\n",
       "       [0.00198855, 0.00832901, 0.00993463, ..., 0.00039381, 0.0263018 ,\n",
       "        0.03657723],\n",
       "       [0.00155745, 0.06449332, 0.08638214, ..., 0.01972472, 0.0587306 ,\n",
       "        0.03280148],\n",
       "       [0.04483974, 0.06736824, 0.00223428, ..., 0.02030664, 0.06246478,\n",
       "        0.00327823]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "5000\n",
      "300\n",
      "(5000, 300)\n"
     ]
    }
   ],
   "source": [
    "print(type(w2v_test_x))\n",
    "print(len(w2v_test_x))\n",
    "print(len(w2v_test_x[0]))\n",
    "print(w2v_test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
