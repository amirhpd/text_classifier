{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pickle\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(\"dataset_train_pp.csv\")\n",
    "test_set = pd.read_csv(\"dataset_test_pp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating input and label\n",
    "train_x=train_set[\"Description\"]\n",
    "test_x=test_set[\"Description\"]\n",
    "\n",
    "train_y=train_set[\"Class Index\"]\n",
    "test_y=test_set[\"Class Index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    tokens_list = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            tokens_list.append(word)\n",
    "    return tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46 s, sys: 280 ms, total: 46.3 s\n",
      "Wall time: 46.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_x_t = test_x.apply(tokenizer).tolist()\n",
    "train_x_t = train_x.apply(tokenizer).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = train_x_t + test_x_t\n",
    "corpus = train_x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3648948"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_len = 0\n",
    "for i in corpus:\n",
    "    corpus_len += len(i)\n",
    "corpus_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 4s, sys: 555 ms, total: 3min 5s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wv = Word2Vec(corpus, size=vec_size, window=5, min_count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89994.213\n"
     ]
    }
   ],
   "source": [
    "p = pickle.dumps(wv)\n",
    "memoryKB = sys.getsizeof(p)/1000\n",
    "print(memoryKB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.save(\"w2v_115.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = wv.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute L2-normalized vectors. You cannot continue training after doing a replace. \n",
    "# The model becomes effectively read-only: you can call most_similar(), similarity(), etc., but not train.\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uvc',\n",
       " 'maui',\n",
       " 'domachowska',\n",
       " 'smb',\n",
       " 'occupancy',\n",
       " 'lastditch',\n",
       " 'collapse',\n",
       " 'legendary',\n",
       " 'lastweek',\n",
       " 'englishlanguage']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view some vocabularies\n",
    "list(islice(wv.vocab, 10, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wv.vectors_norm or wv.syn0norm: unit-normaliyed vector of a vocabulary word, returns a 300 dim vector\n",
    "# gensim.matutils.unitvec : Scale a vector to unit length.\n",
    "def word_averaging(wv, words_list):\n",
    "    avg_all = []\n",
    "    unk = 0\n",
    "    for words in words_list:\n",
    "        norm_words = []\n",
    "        for token in words:\n",
    "            if token in wv.vocab:\n",
    "                norm_tokens = wv.vectors_norm[wv.vocab[token].index]\n",
    "                norm_words.append(norm_tokens)\n",
    "            else:\n",
    "                norm_words.append(np.zeros(vec_size))\n",
    "                unk += 1\n",
    "                \n",
    "        avg_words = gensim.matutils.unitvec(abs(np.array(norm_words).mean(axis=0))).astype(np.float32)   \n",
    "        avg_all.append(avg_words)\n",
    "    print(\"No. of unknown words:\", unk)\n",
    "    return np.array(avg_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of unknown words: 8451\n",
      "CPU times: user 1.59 s, sys: 4.06 ms, total: 1.6 s\n",
      "Wall time: 1.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w2v_test_x = word_averaging(wv, test_x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of unknown words: 57080\n",
      "CPU times: user 13.6 s, sys: 15.8 ms, total: 13.7 s\n",
      "Wall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w2v_train_x = word_averaging(wv, train_x_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save w2v matrix to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"w2v_train_x_115.npy\", w2v_train_x)\n",
    "np.save(\"w2v_test_x_115.npy\", w2v_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test_x = np.load(\"w2v_test_x.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train_x = np.load(\"w2v_train_x.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0583921 , 0.12589933, 0.04027877, ..., 0.08168618, 0.03448169,\n",
       "        0.08376984],\n",
       "       [0.05356431, 0.08018451, 0.01231005, ..., 0.10732461, 0.02249216,\n",
       "        0.05081633],\n",
       "       [0.1155323 , 0.11814706, 0.06095942, ..., 0.13833633, 0.02144026,\n",
       "        0.10468481],\n",
       "       ...,\n",
       "       [0.02837246, 0.03734729, 0.02315114, ..., 0.06863417, 0.05062385,\n",
       "        0.09718889],\n",
       "       [0.01824571, 0.09912425, 0.01130349, ..., 0.06550372, 0.04749638,\n",
       "        0.08120343],\n",
       "       [0.01502722, 0.04912486, 0.0316234 , ..., 0.14490038, 0.06974927,\n",
       "        0.14227726]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "5000\n",
      "300\n",
      "(5000, 300)\n"
     ]
    }
   ],
   "source": [
    "print(type(w2v_test_x))\n",
    "print(len(w2v_test_x))\n",
    "print(len(w2v_test_x[0]))\n",
    "print(w2v_test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
